# -*- coding: utf-8 -*-
"""NLP_final project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SUh2pc2u87uzd7KLGF9IC8l_UGslutkx
"""

!pip install datasets transformers sentence-transformers faiss-cpu python-dotenv openai

import os
import re
import numpy as np
import faiss
from datasets import load_dataset
from sentence_transformers import SentenceTransformer
# The 'openai' library is used, but configured to talk to the Gemini API
from openai import OpenAI, RateLimitError
from google.colab import userdata

# --- START: API KEY AND CLIENT SETUP FOR GEMINI ---

# 1) Get the Gemini API key from Colab secrets.
# IMPORTANT: Replace "GEMINI_API_KEY" with the exact name you use in Colab Secrets
GEMINI_API_KEY = userdata.get("Fifa-25")

# 2) Initialize the OpenAI client, pointing to the Gemini API endpoint
# This tells the library to send requests to Google's service.
client = OpenAI(
    api_key=GEMINI_API_KEY,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

# --- END: API KEY AND CLIENT SETUP FOR GEMINI ---

dataset = load_dataset("squad")
subset = dataset["train"].select(range(500))

def clean_text(text):
    text = text.lower()
    text = re.sub(r"[^a-z0-9\s]", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text

texts = []
for item in subset:
    texts.append(clean_text(item["context"]))
    texts.append(clean_text(item["question"]))

texts[:2]

embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
embeddings = embedding_model.encode(texts)

index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings))

def search_docs(query, k=3):
    query = clean_text(query)
    query_embedding = embedding_model.encode([query])
    # The faiss index requires the query embedding to be 2D (batch size, dimension)
    _, indices = index.search(np.array(query_embedding).astype(np.float32), k)
    return [texts[i] for i in indices[0]]

def rag_chatbot(query):
    try:
        docs = search_docs(query)
        context = "\n".join(docs)

        prompt = f"""Context:
{context}

Question:
{query}
"""
        # --- MODEL NAME CHANGE ---
        # Changed the model name to gemini-2.5-flash, which is compatible
        # with the OpenAI chat completions API through the base_url.
        response = client.chat.completions.create(
            model="gemini-2.5-flash",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.choices[0].message.content

    except RateLimitError:
        return "API quota exceeded. Please check billing or use another API key."
    except Exception as e:
        # Catch other potential errors, such as a bad API key or endpoint issue
        return f"An API Error Occurred: {e}"

while True:
    user_input = input("You: ")
    if user_input.lower() in ["exit", "quit"]:
        print("Bot: Goodbye!")
        break

    answer = rag_chatbot(user_input)
    print("Bot:", answer)+